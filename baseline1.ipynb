{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "baseline1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JE39xqqSx6m"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import copy\n",
        "from glob import glob\n",
        "from random import sample\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fp9ZtEUTMOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3beb6fed-aed2-408b-ac6f-3047e2543d6a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKncWHn4TbJd"
      },
      "source": [
        "# !unzip /content/drive/MyDrive/cil-road-segmentation-2021.zip -d /content/drive/MyDrive/cil-road-segmentation-2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twL096WssE9H"
      },
      "source": [
        "# mean and std of training set\n",
        "mean1 = np.array([0.330, 0.327, 0.293])\n",
        "std1 = np.array([0.183, 0.176, 0.175])\n",
        "\n",
        "# flip the image horizaontally with probability = prob\n",
        "def horizontal_flip(train_img, label_img, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "        return cv2.flip(train_img, 1), cv2.flip(label_img, 1)\n",
        "    else:\n",
        "        return train_img, label_img\n",
        "\n",
        "\n",
        "# flip the image vertically with probability = prob\n",
        "def vertical_flip(train_img, label_img, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "        return cv2.flip(train_img, 0), cv2.flip(label_img, 0)\n",
        "    else:\n",
        "        return train_img, label_img\n",
        "\n",
        "\n",
        "# rotate the image by k*90 degree with probability = prob\n",
        "def rotate_90s(train_img, label_img, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "        # 1<= k <= 3, rotate clockwise by 90/180/270 degree\n",
        "        k = np.random.randint(low=1, high=4, size=1)[0]\n",
        "        return np.rot90(train_img, k), np.rot90(label_img, k)\n",
        "    else:\n",
        "        return train_img, label_img\n",
        "\n",
        "\n",
        "# adjust the hue of an RGB image by random factor in [-10, 10] with probability = prob\n",
        "def hue_image(train_img, label_img, min_hue_factor=-10, max_hue_factor=10, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "        hsv = cv2.cvtColor(train_img, cv2.COLOR_RGB2HSV)\n",
        "        h, s, v = cv2.split(hsv)\n",
        "        delta = np.random.randint(low=min_hue_factor, high=max_hue_factor, size=1)[0]\n",
        "        h = np.clip(h+delta, 0, 180).astype(h.dtype)\n",
        "        final_hsv = cv2.merge((h, s, v))\n",
        "        new_train_img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2RGB)\n",
        "        return new_train_img, label_img\n",
        "    else:\n",
        "        return train_img, label_img\n",
        "\n",
        "\n",
        "# adjust the saturation of an RGB image by random factor in [-20, 20] with probability = prob\n",
        "def saturation_image(train_img, label_img, min_saturation_factor=-20, max_saturation_factor=20, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "        hsv = cv2.cvtColor(train_img, cv2.COLOR_RGB2HSV)\n",
        "        h, s, v = cv2.split(hsv)\n",
        "        delta = np.random.randint(low=min_saturation_factor, high=max_saturation_factor, size=1)[0]\n",
        "        s = np.clip(s+delta, 0, 255).astype(h.dtype)\n",
        "        final_hsv = cv2.merge((h, s, v))\n",
        "        # print(h.shape)\n",
        "        # print(v.shape)\n",
        "        # print(s.shape)\n",
        "        new_train_img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2RGB)\n",
        "        return new_train_img, label_img\n",
        "    else:\n",
        "        return train_img, label_img\n",
        "\n",
        "\n",
        "# adjust the brightness of an RGB image by random delta in [-30, 30] with probability = prob\n",
        "def brightness_image(train_img, label_img, min_brightness_factor=-30, max_brightness_factor=30, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "        brightness_factor = np.random.uniform(min_brightness_factor, max_brightness_factor, 1)[0]\n",
        "        new_train_img = _contrast_and_brightness(train_img, 1, brightness_factor)\n",
        "        return new_train_img, label_img\n",
        "    else:\n",
        "        return train_img, label_img\n",
        "\n",
        "\n",
        "# adjust the contrast of an RGB image by random factor in [0.6, 2.5] with probability = prob\n",
        "def contrast_image(train_img, label_img, min_contrast_factor=0.6, max_contrast_factor=2.5, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "        contrast_factor = np.random.uniform(min_contrast_factor, max_contrast_factor, 1)[0]\n",
        "        new_train_img = _contrast_and_brightness(train_img, contrast_factor, 0)\n",
        "        return new_train_img, label_img\n",
        "    else:\n",
        "        return train_img, label_img\n",
        "\n",
        "\n",
        "def _contrast_and_brightness(img, contrast_factor, brightness_factor):\n",
        "    blank = np.zeros(img.shape, img.dtype)\n",
        "    dst = cv2.addWeighted(img, contrast_factor, blank, 1-contrast_factor, brightness_factor)\n",
        "    return dst\n",
        "\n",
        "\n",
        "def random_scale(train_img, label_img, pad_reflect=False, probs=[0.7, 0.9, 1]):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < probs[0]:\n",
        "        return _crop_and_scale_up(train_img, label_img)\n",
        "    elif rdn < probs[1]:\n",
        "        return _random_shift(train_img, label_img)\n",
        "    else:\n",
        "        return _shrink_and_pad(train_img, label_img, pad_reflect)\n",
        "\n",
        "\n",
        "def _crop_and_scale_up(train_img, label_img, crop_size=[(200, 200), (250, 250), (300, 300), (350, 350)]):\n",
        "    original_shape = train_img.shape[:2]\n",
        "\n",
        "    # cropping\n",
        "    random_crop_shape = random.choice(crop_size)\n",
        "    train_img, label_img = _random_crop(train_img, label_img, random_crop_shape)\n",
        "\n",
        "    # scalse up\n",
        "    train_img = cv2.resize(train_img, original_shape, interpolation=cv2.INTER_LINEAR)\n",
        "    label_img = cv2.resize(label_img, original_shape, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    return train_img, label_img\n",
        "\n",
        "\n",
        "def _random_crop(train_img, label_img, crop_shape):\n",
        "    original_shape = train_img.shape[:2]\n",
        "\n",
        "    crop_h = original_shape[0]-crop_shape[0]\n",
        "    crop_w = original_shape[1]-crop_shape[1]\n",
        "    nh = random.randint(0, crop_h)\n",
        "    nw = random.randint(0, crop_w)\n",
        "    train_crop = train_img[nh:nh + crop_shape[0], nw:nw + crop_shape[1]]\n",
        "    label_crop = label_img[nh:nh + crop_shape[0], nw:nw + crop_shape[1]]\n",
        "    return train_crop, label_crop\n",
        "\n",
        "\n",
        "def _random_shift(train_img, label_img):\n",
        "    original_shape = train_img.shape[:2]\n",
        "    max_translation = np.multiply(0.15, original_shape).astype(np.int64)\n",
        "\n",
        "    delta_h = random.randint(-max_translation[0], max_translation[0])\n",
        "    delta_w = random.randint(-max_translation[1], max_translation[1])\n",
        "\n",
        "    train_img = _shift(_shift(train_img, delta_h, height=True), delta_w, height=False)\n",
        "    label_img = _shift(_shift(label_img, delta_h, height=True), delta_w, height=False)\n",
        "\n",
        "    return train_img, label_img\n",
        "\n",
        "\n",
        "def _shift(img, delta, height):\n",
        "    if delta == 0:\n",
        "        return img\n",
        "    translated_img = np.empty_like(img)\n",
        "    if height:\n",
        "        if delta >= 0:\n",
        "            translated_img[:delta] = 0\n",
        "            translated_img[delta:] = img[:-delta]\n",
        "        elif delta < 0:\n",
        "            translated_img[:delta] = img[-delta:]\n",
        "            translated_img[delta:] = 0\n",
        "        return translated_img\n",
        "    else:\n",
        "        if delta >= 0:\n",
        "            translated_img[:, :delta] = 0\n",
        "            translated_img[:, delta:] = img[:, :-delta]\n",
        "        elif delta < 0:\n",
        "            translated_img[:, :delta] = img[:, -delta:]\n",
        "            translated_img[:, delta:] = 0\n",
        "        return translated_img\n",
        "\n",
        "\n",
        "def _shrink_and_pad(train_img, label_img, pad_reflect, shrink_range=(0.6, 0.95)):\n",
        "    original_shape = train_img.shape[:2]\n",
        "\n",
        "    random_ratio = np.random.uniform(shrink_range[0], shrink_range[1])\n",
        "    train_img, label_img = _random_shrink(train_img, label_img, random_ratio)\n",
        "    train_img, label_img = _random_pad(train_img, label_img, original_shape, pad_reflect)\n",
        "\n",
        "    return train_img, label_img\n",
        "\n",
        "\n",
        "def _random_shrink(train_img, label_img, ratio):\n",
        "    original_shape = train_img.shape[:2]\n",
        "    shrink_shape = (int(original_shape[0]*ratio), int(original_shape[1]*ratio))\n",
        "    # shrink\n",
        "    train_img = cv2.resize(train_img, shrink_shape, interpolation=cv2.INTER_LINEAR)\n",
        "    label_img = cv2.resize(label_img, shrink_shape, interpolation=cv2.INTER_LINEAR)\n",
        "    return train_img, label_img\n",
        "\n",
        "\n",
        "def _random_pad(train_img, label_img, target_shape, pad_reflect):\n",
        "    original_shape = train_img.shape[:2]\n",
        "\n",
        "    # put to center and padding\n",
        "    margin = np.subtract(target_shape, original_shape)\n",
        "\n",
        "    # random translation: limited by max_ratio and remained margin\n",
        "    max_translation = np.multiply(0.15, original_shape)\n",
        "    max_translation = np.minimum((margin // 2), max_translation)\n",
        "    max_translation = max_translation.astype(np.int64)\n",
        "\n",
        "    # place image with random translation\n",
        "    pad_top = margin[0] // 2 + random.randint(-max_translation[0], max_translation[0])\n",
        "    pad_left = margin[1] // 2 + random.randint(-max_translation[1], max_translation[1])\n",
        "    pad_bottom = margin[0] - pad_top\n",
        "    pad_right = margin[1] - pad_left\n",
        "\n",
        "    # padding to original size\n",
        "    if pad_reflect:\n",
        "        train_img = cv2.copyMakeBorder(train_img, pad_top, pad_bottom, pad_left, pad_right, cv2.BORDER_REFLECT)\n",
        "        label_img = cv2.copyMakeBorder(label_img, pad_top, pad_bottom, pad_left, pad_right, cv2.BORDER_REFLECT)\n",
        "    else:\n",
        "        train_img = cv2.copyMakeBorder(train_img, pad_top, pad_bottom, pad_left, pad_right, cv2.BORDER_CONSTANT, value=0)\n",
        "        label_img = cv2.copyMakeBorder(label_img, pad_top, pad_bottom, pad_left, pad_right, cv2.BORDER_CONSTANT, value=0)\n",
        "\n",
        "    return train_img, label_img\n",
        "\n",
        "\n",
        "# rotate the image by a minor degree in [+25, -25] with probability = prob\n",
        "def random_rotate(train_img, label_img, min_angle=-25, max_angle=25, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "        random_angle = np.random.uniform(min_angle, max_angle, 1)[0]\n",
        "        return _rotate_image(train_img, random_angle), _rotate_image(label_img, random_angle)\n",
        "    else:\n",
        "        return train_img, label_img\n",
        "        # return tf.convert_to_tensor(train_img), tf.convert_to_tensor(label_img)\n",
        "\n",
        "\n",
        "def _rotate_image(img, angle):\n",
        "    if -1 < angle < 1:\n",
        "        return img\n",
        "    shape_2d = (img.shape[1], img.shape[0])\n",
        "    center_2d = (img.shape[1] / 2, img.shape[0] / 2)\n",
        "    rotation_matrix = cv2.getRotationMatrix2D(center_2d, angle, 1.0)\n",
        "    img = cv2.warpAffine(img, rotation_matrix, shape_2d, flags=cv2.INTER_LINEAR)\n",
        "    return img\n",
        "\n",
        "\n",
        "def normalize(img):\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    img = img - mean1\n",
        "    img = img / std1\n",
        "    return img\n",
        "\n",
        "\n",
        "def discretize(gt, threshold=40):\n",
        "    # The order matters\n",
        "    gt[gt < threshold] = 0\n",
        "    gt[gt >= threshold] = 1\n",
        "    return gt\n",
        "\n",
        "\n",
        "def get_edge_mask(image):\n",
        "    \"\"\" Accept image before binarization \"\"\"\n",
        "    edge_mask = cv2.Canny(image, 0, 255)\n",
        "    edge_mask[image < 40] = 0\n",
        "    edge_mask[edge_mask != 0] = 1\n",
        "    return edge_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_7jn7d1sUaa"
      },
      "source": [
        "# from google.colab.patches import cv2_imshow\n",
        "\n",
        "# train_img = cv2.imread('/content/drive/MyDrive/cil-project/cil-road-segmentation-2021/training/images/satImage_001.png')\n",
        "# label_img = cv2.imread('/content/drive/MyDrive/cil-project/cil-road-segmentation-2021/training/groundtruth/satImage_001.png')\n",
        "# imgs = np.hstack([train_img,label_img])\n",
        "# cv2_imshow(imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdP-eI7Bj5bJ"
      },
      "source": [
        "# !unzip /content/drive/MyDrive/cil-project/cil-road-segmentation-2021.zip -d /content/drive/MyDrive/cil-project/cil-road-segmentation-2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug6wFhTZ6-0Q"
      },
      "source": [
        "# some constants\n",
        "PATCH_SIZE = 16  # pixels per side of square patches\n",
        "VAL_SIZE = 10  # size of the validation set (number of images)\n",
        "CUTOFF = 0.25  # minimum average brightness for a mask patch to be classified as containing road"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoEFsuB4Sx6q"
      },
      "source": [
        "# # unzip the dataset, split it and organize it in folders\n",
        "# if not os.path.isdir('validation'):  # make sure this has not been executed yet\n",
        "#   try:\n",
        "#           # !unzip /content/drive/MyDrive/cil-project/cil-road-segmentation-2021.zip -d cil-road-segmentation-2021\n",
        "#           # !mv /content/drive/MyDrive/cil-project/cil-road-segmentation-2021/training/training/* /content/drive/MyDrive/cil-project/cil-road-segmentation-2021/training\n",
        "#           # !rm -rf /content/drive/MyDrive/cil-project/cil-road-segmentation-2021/training/training\n",
        "#           # !mkdir /content/drive/MyDrive/cil-project/cil-road-segmentation-2021/validation\n",
        "#           # !mkdir /content/drive/MyDrive/cil-project/cil-road-segmentation-2021/validation/images\n",
        "#           # !mkdir /content/drive/MyDrive/cil-project/cil-road-segmentation-2021/validation/groundtruth\n",
        "#           for img in sample(glob(\"/content/drive/MyDrive/cil-project/cil-road-segmentation-2021/training/images/*.png\"), VAL_SIZE):\n",
        "#             os.rename(img, img.replace('/content/drive/MyDrive/cil-project/cil-road-segmentation-2021/training', '/content/drive/MyDrive/cil-project/cil-road-segmentation-2021/validation'))\n",
        "#             mask = img.replace('images', 'groundtruth')\n",
        "#             os.rename(mask, mask.replace('/content/drive/MyDrive/cil-project/cil-road-segmentation-2021/training', '/content/drive/MyDrive/cil-project/cil-road-segmentation-2021/validation'))\n",
        "#   except:\n",
        "#       print('Please upload a .zip file containing your datasets.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGpi99JASx6r"
      },
      "source": [
        "def load_all_from_path_255(path):\n",
        "    # loads all HxW .pngs contained in path as a 4D np.array of shape (n_images, H, W, 3)\n",
        "    # images are loaded as floats with values in the interval [0., 1.]\n",
        "    return np.stack([np.array(Image.open(f)) for f in sorted(glob(path + '/*.png'))]).astype(np.float32)\n",
        "\n",
        "def load_all_from_path(path):\n",
        "    # loads all HxW .pngs contained in path as a 4D np.array of shape (n_images, H, W, 3)\n",
        "    # images are loaded as floats with values in the interval [0., 1.]\n",
        "    return np.stack([np.array(Image.open(f)) for f in sorted(glob(path + '/*.png'))]).astype(np.float32) / 255.\n",
        "\n",
        "# paths to training and validation datasets\n",
        "train_path = '/content/drive/MyDrive/cil-project/cil-road-segmentation-2021/training'\n",
        "val_path = '/content/drive/MyDrive/cil-project/cil-road-segmentation-2021/validation'\n",
        "\n",
        "train_images = load_all_from_path_255(os.path.join(train_path, 'images'))\n",
        "train_masks = load_all_from_path_255(os.path.join(train_path, 'groundtruth'))\n",
        "val_images = load_all_from_path_255(os.path.join(val_path, 'images'))\n",
        "val_masks = load_all_from_path_255(os.path.join(val_path, 'groundtruth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51bhwOjU-vi0",
        "outputId": "afb6f3c9-bb49-481e-f502-8b5caff2be2c"
      },
      "source": [
        "print(train_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90, 400, 400, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK7DQFxExbmu"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "from google.colab.patches import cv2_imshow\n",
        "def pre_process_and_save_images(train_images, label_images):\n",
        "  transformed_train_images = np.zeros(train_images.shape, dtype=np.float32)\n",
        "  transformed_label_images = np.zeros(label_images.shape, dtype=np.float32)\n",
        "  cnt = 0\n",
        "  for train_image, label_image in zip(train_images, label_images):\n",
        "      # train_image, label_image = horizontal_flip(train_image, label_image)\n",
        "      # train_image, label_image = vertical_flip(train_image, label_image)\n",
        "      # train_image, label_image = rotate_90s(train_image, label_image)\n",
        "      train_image, label_image = random_rotate(train_image, label_image)\n",
        "      # train_image, label_image = random_scale(train_image, label_image)\n",
        "      # train_image, label_image = hue_image(train_image, label_image)\n",
        "      # train_image, label_image = saturation_image(train_image, label_image)\n",
        "      # train_image, label_image = brightness_image(train_image, label_image)\n",
        "      # train_image, label_image = contrast_image(train_image, label_image)\n",
        "      transformed_train_images[cnt] = train_image\n",
        "      transformed_label_images[cnt] = label_image\n",
        "      cnt += 1\n",
        "  return np.vstack((train_images, transformed_train_images)), np.vstack((label_images, transformed_label_images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5Kk6w4GzZgN",
        "outputId": "81ba64cc-8c0f-4a9c-9861-6392bc45a035"
      },
      "source": [
        "train_images, train_masks = pre_process_and_save_images(train_images, train_masks)\n",
        "val_images_0, val_masks_0 = pre_process_and_save_images(val_images, val_masks)\n",
        "val_images, val_masks = pre_process_and_save_images(val_images_0, val_masks_0)\n",
        "print(val_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40, 400, 400, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSP1FIm-wahV",
        "outputId": "124b21b5-4719-46da-b95c-1b7183941139"
      },
      "source": [
        "val_images, val_masks = pre_process_and_save_images(val_images, val_masks)\n",
        "print(val_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(80, 400, 400, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mPh86h4p71u"
      },
      "source": [
        "folder_name = '/content/drive/MyDrive/cil-project/cil-road-segmentation-2021/original_random_rotate'\n",
        "num_val = str(val_images.shape[0])\n",
        "# !mkdir $folder_name\n",
        "# np.save(folder_name+'/train_images.npy',train_images)\n",
        "# np.save(folder_name+'/train_masks.npy',train_masks)\n",
        "np.save(folder_name+'/val_images.npy',val_images)\n",
        "np.save(folder_name+'/val_masks.npy',val_masks)\n",
        "# !mkdir $folder_name/model\n",
        "# !mkdir $folder_name/predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1bJQIcAztbW"
      },
      "source": [
        "folder_name = '/content/drive/MyDrive/cil-project/cil-road-segmentation-2021/original_random_rotate'\n",
        "num_val = str(val_images.shape[0])\n",
        "# !rm -rf $folder_name\n",
        "!mkdir $folder_name\n",
        "np.save(folder_name+'/train_images.npy',train_images)\n",
        "np.save(folder_name+'/train_masks.npy',train_masks)\n",
        "np.save(folder_name+'/val_images.npy',val_images)\n",
        "np.save(folder_name+'/val_masks.npy',val_masks)\n",
        "!mkdir $folder_name/model\n",
        "!mkdir $folder_name/predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-F4YFqf3SVw"
      },
      "source": [
        "def from_array_to_pictures(pictures_array, path_suffix):\n",
        "  cnt = 0\n",
        "  for a in pictures_array:\n",
        "    path = folder_name+path_suffix\n",
        "    cv2.imwrite(path + str(cnt) + '.png', a)\n",
        "    cnt += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6RMk3uR2meM"
      },
      "source": [
        "# !rm -rf $folder_name/training\n",
        "# !rm -rf $folder_name/training/images\n",
        "# !rm -rf $folder_name/training/groundtruth\n",
        "!rm -rf $folder_name/validation\n",
        "# !rm -rf $folder_name/validation/images\n",
        "# !rm -rf $folder_name/validation/groundtruth\n",
        "# !mkdir $folder_name/training\n",
        "# !mkdir $folder_name/training/images\n",
        "# !mkdir $folder_name/training/groundtruth\n",
        "!mkdir $folder_name/validation\n",
        "!mkdir $folder_name/validation/images\n",
        "!mkdir $folder_name/validation/groundtruth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QifNwmZz4Hcq"
      },
      "source": [
        "# from_array_to_pictures(train_images, '/training/images/')\n",
        "# from_array_to_pictures(train_masks, '/training/groundtruth/')\n",
        "from_array_to_pictures(val_images, '/validation/images/')\n",
        "from_array_to_pictures(val_masks, '/validation/groundtruth/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyyhhvqCSx6w"
      },
      "source": [
        "pip install tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp-ZmoZzGOeA"
      },
      "source": [
        "# import tensorflow.compat.v1 as tf\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# def pre_process_images(train_images, label_images):\n",
        "#   transformed_train_images = np.zeros(train_images.shape, dtype=np.float32)\n",
        "#   transformed_label_images = np.zeros(label_images.shape, dtype=np.float32)\n",
        "#   cnt = 0\n",
        "#   with tf.Session() as sess:\n",
        "#     for train_image, label_image in zip(train_images, label_images):\n",
        "#       # train_image, label_image = horizontal_flip(train_image, label_image)\n",
        "#       # train_image, label_image = vertical_flip(train_image, label_image)\n",
        "#       # train_image, label_image = rotate_90s(train_image, label_image)\n",
        "#       # train_image, label_image = random_rotate(train_image, label_image)\n",
        "#       train_image, label_image = random_scale(train_image, label_image)\n",
        "#       # train_image, label_image = hue_image(train_image, label_image)\n",
        "#       # train_image, label_image = saturation_image(train_image, label_image)\n",
        "#       # train_image, label_image = brightness_image(train_image, label_image)\n",
        "#       # train_image, label_image = contrast_image(train_image, label_image)\n",
        "#       transformed_train_images[cnt] = train_image\n",
        "#       transformed_label_images[cnt] = label_image\n",
        "#       cnt += 1\n",
        "#   return transformed_train_images, transformed_label_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY16_IIDSx6t"
      },
      "source": [
        "def image_to_patches(images, masks=None):\n",
        "    # takes in a 4D np.array containing images and (optionally) a 4D np.array containing the segmentation masks\n",
        "    # returns a 4D np.array with an ordered sequence of patches extracted from the image and (optionally) a np.array containing labels\n",
        "    n_images = images.shape[0]  # number of images\n",
        "    h, w = images.shape[1:3]  # shape of images\n",
        "    assert (h % PATCH_SIZE) + (w % PATCH_SIZE) == 0  # make sure images can be patched exactly\n",
        "\n",
        "    h_patches = h // PATCH_SIZE\n",
        "    w_patches = w // PATCH_SIZE\n",
        "    patches = images.reshape((n_images, h_patches, PATCH_SIZE, h_patches, PATCH_SIZE, -1))\n",
        "    patches = np.moveaxis(patches, 2, 3)\n",
        "    patches = patches.reshape(-1, PATCH_SIZE, PATCH_SIZE, 3)\n",
        "    if masks is None:\n",
        "        return patches\n",
        "\n",
        "    masks = masks.reshape((n_images, h_patches, PATCH_SIZE, h_patches, PATCH_SIZE, -1))\n",
        "    masks = np.moveaxis(masks, 2, 3)\n",
        "    labels = np.mean(masks, (-1, -2, -3)) > CUTOFF  # compute labels\n",
        "    labels = labels.reshape(-1).astype(np.float32)\n",
        "    return patches, labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMkJo0nZSx6w"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def np_to_tensor(x, device):\n",
        "    # allocates tensors from np.arrays\n",
        "    if device == 'cpu':\n",
        "        return torch.from_numpy(x).cpu()\n",
        "    else:\n",
        "        return torch.from_numpy(x).contiguous().pin_memory().to(device=device, non_blocking=True)\n",
        "\n",
        "def accuracy_fn(y_hat, y):\n",
        "    # computes classification accuracy\n",
        "    return (y_hat.round() == y.round()).float().mean()\n",
        "\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    # dataset class that deals with loading the data and making it available by index.\n",
        "\n",
        "    def __init__(self, path, device, use_patches=True, resize_to=(400, 400)):\n",
        "        self.path = path\n",
        "        self.device = device\n",
        "        self.use_patches = use_patches\n",
        "        self.resize_to=resize_to\n",
        "        self.x, self.y, self.n_samples = None, None, None\n",
        "        self._load_data()\n",
        "\n",
        "    def _load_data(self):  # not very scalable, but good enough for now\n",
        "        self.x = load_all_from_path(os.path.join(self.path, 'images'))\n",
        "        self.y = load_all_from_path(os.path.join(self.path, 'groundtruth'))\n",
        "        # self.x, self.y = pre_process_images(self.x, self.y)\n",
        "        if self.use_patches:  # split each image into patches\n",
        "            self.x, self.y = image_to_patches(self.x, self.y)\n",
        "        elif self.resize_to != (self.x.shape[1], self.x.shape[2]):  # resize images\n",
        "            self.x = np.stack([cv2.resize(img, dsize=self.resize_to) for img in self.x], 0)\n",
        "            self.y = np.stack([cv2.resize(mask, dsize=self.resize_to) for mask in self.y], 0)\n",
        "        self.x = np.moveaxis(self.x, -1, 1)  # pytorch works with CHW format instead of HWC\n",
        "        self.n_samples = len(self.x)\n",
        "\n",
        "    def _preprocess(self, x, y):\n",
        "        # to keep things simple we will not apply transformations to each sample,\n",
        "        # but it would be a very good idea to look into preprocessing\n",
        "        return x, y\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self._preprocess(np_to_tensor(self.x[item], self.device), np_to_tensor(self.y[[item]], self.device))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s-4fJ46Sx6y"
      },
      "source": [
        "class Block(nn.Module):\n",
        "    # a repeating structure composed of two convolutional layers with batch normalization and ReLU activations\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.BatchNorm2d(out_ch),\n",
        "                                   nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
        "                                   nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "        \n",
        "class UNet(nn.Module):\n",
        "    # UNet-like architecture for single class semantic segmentation.\n",
        "    def __init__(self, chs=(3,64,128,256,512,1024)):\n",
        "        super().__init__()\n",
        "        enc_chs = chs  # number of channels in the encoder\n",
        "        dec_chs = chs[::-1][:-1]  # number of channels in the decoder\n",
        "        self.enc_blocks = nn.ModuleList([Block(in_ch, out_ch) for in_ch, out_ch in zip(enc_chs[:-1], enc_chs[1:])])  # encoder blocks\n",
        "        self.pool = nn.MaxPool2d(2)  # pooling layer (can be reused as it will not be trained)\n",
        "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(in_ch, out_ch, 2, 2) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # deconvolution\n",
        "        self.dec_blocks = nn.ModuleList([Block(in_ch, out_ch) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # decoder blocks\n",
        "        self.head = nn.Sequential(nn.Conv2d(dec_chs[-1], 1, 1), nn.Sigmoid()) # 1x1 convolution for producing the output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encode\n",
        "        enc_features = []\n",
        "        for block in self.enc_blocks[:-1]:\n",
        "            x = block(x)  # pass through the block\n",
        "            enc_features.append(x)  # save features for skip connections\n",
        "            x = self.pool(x)  # decrease resolution\n",
        "        x = self.enc_blocks[-1](x)\n",
        "        # decode\n",
        "        for block, upconv, feature in zip(self.dec_blocks, self.upconvs, enc_features[::-1]):\n",
        "            x = upconv(x)  # increase resolution\n",
        "            x = torch.cat([x, feature], dim=1)  # concatenate skip features\n",
        "            x = block(x)  # pass through the block\n",
        "        return self.head(x)  # reduce to 1 channel\n",
        "\n",
        "\n",
        "def patch_accuracy_fn(y_hat, y):\n",
        "    # computes accuracy weighted by patches (metric used on Kaggle for evaluation)\n",
        "    h_patches = y.shape[-2] // PATCH_SIZE\n",
        "    w_patches = y.shape[-1] // PATCH_SIZE\n",
        "    patches_hat = y_hat.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
        "    patches = y.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
        "    return (patches == patches_hat).float().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJa7THIISx6z"
      },
      "source": [
        "def train(train_dataloader, eval_dataloader, model, loss_fn, metric_fns, optimizer, n_epochs):\n",
        "    # training loop\n",
        "    logdir = '/content/drive/MyDrive/cil-project/tensorboard/net'\n",
        "    writer = SummaryWriter(logdir)  # tensorboard writer (can also log images)\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    history = {}  # collects metrics at the end of each epoch\n",
        "\n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        # initialize metric list\n",
        "        metrics = {'loss': [], 'val_loss': []}\n",
        "        for k, _ in metric_fns.items():\n",
        "            metrics[k] = []\n",
        "            metrics['val_'+k] = []\n",
        "\n",
        "        pbar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{n_epochs}')\n",
        "        # training\n",
        "        model.train()\n",
        "        for (x, y) in pbar:\n",
        "            optimizer.zero_grad()  # zero out gradients\n",
        "            y_hat = model(x)  # forward pass\n",
        "            loss = loss_fn(y_hat, y)\n",
        "            loss.backward()  # backward pass\n",
        "            optimizer.step()  # optimize weights\n",
        "\n",
        "            # log partial metrics\n",
        "            metrics['loss'].append(loss.item())\n",
        "            for k, fn in metric_fns.items():\n",
        "                metrics[k].append(fn(y_hat, y).item())\n",
        "            pbar.set_postfix({k: sum(v)/len(v) for k, v in metrics.items() if len(v) > 0})\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():  # do not keep track of gradients\n",
        "            for (x, y) in eval_dataloader:\n",
        "                y_hat = model(x)  # forward pass\n",
        "                loss = loss_fn(y_hat, y)\n",
        "                \n",
        "                # log partial metrics\n",
        "                metrics['val_loss'].append(loss.item())\n",
        "                for k, fn in metric_fns.items():\n",
        "                    metrics['val_'+k].append(fn(y_hat, y).item())\n",
        "\n",
        "        # summarize metrics, log to tensorboard and display\n",
        "        history[epoch] = {k: sum(v) / len(v) for k, v in metrics.items()}\n",
        "        for k, v in history[epoch].items():\n",
        "          writer.add_scalar(k, v, epoch)\n",
        "        print(' '.join(['\\t- '+str(k)+' = '+str(v)+'\\n ' for (k, v) in history[epoch].items()]))\n",
        "        #show_val_samples(x.detach().cpu().numpy(), y.detach().cpu().numpy(), y_hat.detach().cpu().numpy())\n",
        "        \n",
        "        # deep copy the model\n",
        "        if history[epoch]['val_acc'] > best_acc:\n",
        "          # print(history[epoch]['val_acc'])\n",
        "          best_acc = history[epoch]['val_acc']\n",
        "          best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print('Finished Training')\n",
        "    print(best_acc)\n",
        "    torch.save(model.state_dict(),folder_name+'/model/model_e'+str(n_epochs)+'_val'+num_val+'.pt')\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    torch.save(model.state_dict(),folder_name+'/model/best_val_acc_model_e'+str(n_epochs)+'_val'+num_val+'.pt')\n",
        "    # plot loss curves\n",
        "    plt.plot([v['loss'] for k, v in history.items()], label='Training Loss')\n",
        "    plt.plot([v['val_loss'] for k, v in history.items()], label='Validation Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEScJufKSx60"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# reshape the image to simplify the handling of skip connections and maxpooling\n",
        "train_dataset = ImageDataset(folder_name+'/training', device, use_patches=False, resize_to=(384, 384))\n",
        "val_dataset = ImageDataset(folder_name+'/validation', device, use_patches=False, resize_to=(384, 384))\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "model = UNet().to(device)\n",
        "loss_fn = nn.BCELoss()\n",
        "metric_fns = {'acc': accuracy_fn, 'patch_acc': patch_accuracy_fn}\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "n_epochs = 60"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMbODWJN718O"
      },
      "source": [
        "train(train_dataloader, val_dataloader, model, loss_fn, metric_fns, optimizer, n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgwDyGOf6nSL"
      },
      "source": [
        "# model.load_state_dict(torch.load(folder_name+'/model/model_e'+str(n_epochs)+'_val'+num_val+'.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKcowcgjAVC0",
        "outputId": "23b03413-5bff-46a0-f2c9-b498d41103e8"
      },
      "source": [
        "model.load_state_dict(torch.load(folder_name+'/model/best_val_acc_model_e'+str(n_epochs)+'_val'+num_val+'.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na5Z8MW0P_e0"
      },
      "source": [
        "test_path = '/content/drive/MyDrive/cil-project/cil-road-segmentation-2021/test_images/test_images'\n",
        "def create_submission(labels,test_filenames,submission_filename):\n",
        "  with open(submission_filename,'w') as f:\n",
        "    f.write('id,prediction\\n')\n",
        "    for fn, patch_array in zip(sorted(test_filenames), test_pred):\n",
        "      img_number = int(re.findall(r\"\\d+\", fn)[-1])\n",
        "      for i in range(patch_array.shape[0]):\n",
        "        for j in range(patch_array.shape[1]):\n",
        "          f.write(\"{:03d}_{}_{},{}\\n\".format(img_number, i*PATCH_SIZE, j*PATCH_SIZE, int(patch_array[j, i])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAjFMs9lLciD"
      },
      "source": [
        "# predict on test set\n",
        "test_filenames = (glob(test_path + '/*.png'))\n",
        "test_images = load_all_from_path(test_path)\n",
        "batch_size = test_images.shape[0]\n",
        "size = test_images.shape[1:3]\n",
        "# we also need to resize the test images. This might not be the best ideas depending on their spatial resolution.\n",
        "test_images = np.stack([cv2.resize(img, dsize=(384, 384)) for img in test_images], 0)\n",
        "test_images = np_to_tensor(np.moveaxis(test_images, -1, 1), device)\n",
        "test_pred = [model(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
        "test_pred = np.concatenate(test_pred, 0)\n",
        "test_pred= np.moveaxis(test_pred, 1, -1)  # CHW to HWC\n",
        "test_pred = np.stack([cv2.resize(img, dsize=size) for img in test_pred], 0)  # resize to original shape\n",
        "# now compute labels\n",
        "test_pred = test_pred.reshape((-1, size[0] // PATCH_SIZE, PATCH_SIZE, size[0] // PATCH_SIZE, PATCH_SIZE))\n",
        "test_pred = np.moveaxis(test_pred, 2, 3)\n",
        "test_pred = np.round(np.mean(test_pred, (-1, -2)) > CUTOFF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHoRU8Mb8Bi2"
      },
      "source": [
        "create_submission(test_pred, test_filenames, submission_filename=folder_name+'/predict/unet_submission_e'+str(n_epochs)+'_val'+num_val+'_best.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsrds2nkSx61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98746854-23b7-4e84-9adb-ef50e9cd453b"
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "print(input, input.shape)\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "print(target, target.shape)\n",
        "output = loss(input, target)\n",
        "print(output)\n",
        "output.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1612, -1.1411,  0.0977,  1.4339,  0.3903],\n",
            "        [-0.1963, -0.8168,  0.4892,  0.2308, -0.6857],\n",
            "        [ 0.7305, -0.3553,  0.5998, -1.6141,  0.8353]], requires_grad=True) torch.Size([3, 5])\n",
            "tensor([3, 1, 3]) torch.Size([3])\n",
            "tensor(2.1896, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qf03LTPSx62"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}