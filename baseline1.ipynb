{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "baseline1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiD6f2cbd1uR"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from random import sample\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8thnMdXLeHvC"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9ZJw8Ned1uU"
      },
      "source": [
        "# some constants\n",
        "PATCH_SIZE = 16  # pixels per side of square patches\n",
        "VAL_SIZE = 10  # size of the validation set (number of images)\n",
        "CUTOFF = 0.25  # minimum average brightness for a mask patch to be classified as containing road\n",
        "\n",
        "# unzip the dataset, split it and organize it in folders\n",
        "if not os.path.isdir('validation'):  # make sure this has not been executed yet\n",
        "  try:\n",
        "          #!unzip cil-road-segmentation-2021.zip\n",
        "          !mv training/training/* training\n",
        "          !rm -rf training/training\n",
        "          !mkdir validation\n",
        "          !mkdir validation/images\n",
        "          !mkdir validation/groundtruth\n",
        "          for img in sample(glob(\"training/images/*.png\"), VAL_SIZE):\n",
        "            os.rename(img, img.replace('training', 'validation'))\n",
        "            mask = img.replace('images', 'groundtruth')\n",
        "            os.rename(mask, mask.replace('training', 'validation'))\n",
        "  except:\n",
        "      print('Please upload a .zip file containing your datasets.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXFfpZKId1uV"
      },
      "source": [
        "def load_all_from_path(path):\n",
        "    # loads all HxW .pngs contained in path as a 4D np.array of shape (n_images, H, W, 3)\n",
        "    # images are loaded as floats with values in the interval [0., 1.]\n",
        "    return np.stack([np.array(Image.open(f)) for f in sorted(glob(path + '/*.png'))]).astype(np.float32) / 255.\n",
        "\n",
        "# paths to training and validation datasets\n",
        "train_path = 'training'\n",
        "val_path = 'validation'\n",
        "\n",
        "train_images = load_all_from_path(os.path.join(train_path, 'images'))\n",
        "train_masks = load_all_from_path(os.path.join(train_path, 'groundtruth'))\n",
        "val_images = load_all_from_path(os.path.join(val_path, 'images'))\n",
        "val_masks = load_all_from_path(os.path.join(val_path, 'groundtruth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjmTkUOPd1uW"
      },
      "source": [
        "def image_to_patches(images, masks=None):\n",
        "    # takes in a 4D np.array containing images and (optionally) a 4D np.array containing the segmentation masks\n",
        "    # returns a 4D np.array with an ordered sequence of patches extracted from the image and (optionally) a np.array containing labels\n",
        "    n_images = images.shape[0]  # number of images\n",
        "    h, w = images.shape[1:3]  # shape of images\n",
        "    assert (h % PATCH_SIZE) + (w % PATCH_SIZE) == 0  # make sure images can be patched exactly\n",
        "\n",
        "    h_patches = h // PATCH_SIZE\n",
        "    w_patches = w // PATCH_SIZE\n",
        "    patches = images.reshape((n_images, h_patches, PATCH_SIZE, h_patches, PATCH_SIZE, -1))\n",
        "    patches = np.moveaxis(patches, 2, 3)\n",
        "    patches = patches.reshape(-1, PATCH_SIZE, PATCH_SIZE, 3)\n",
        "    if masks is None:\n",
        "        return patches\n",
        "\n",
        "    masks = masks.reshape((n_images, h_patches, PATCH_SIZE, h_patches, PATCH_SIZE, -1))\n",
        "    masks = np.moveaxis(masks, 2, 3)\n",
        "    labels = np.mean(masks, (-1, -2, -3)) > CUTOFF  # compute labels\n",
        "    labels = labels.reshape(-1).astype(np.float32)\n",
        "    return patches, labels\n",
        "\n",
        "train_patches, train_labels = image_to_patches(train_images, train_masks)\n",
        "val_patches, val_labels = image_to_patches(val_images, val_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68jrbdVad1uX",
        "collapsed": true,
        "outputId": "7a7d7afa-2dd6-4661-d024-69eaa11b62e4"
      },
      "source": [
        "pip install tensorboard"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 7.6 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /Users/shihaitong/opt/anaconda3/lib/python3.8/site-packages (from tensorboard) (1.19.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /Users/shihaitong/opt/anaconda3/lib/python3.8/site-packages (from tensorboard) (0.35.1)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "  Downloading google_auth-1.30.2-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 17.1 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 21.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting protobuf>=3.6.0\n",
            "  Downloading protobuf-3.17.3-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 25.1 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /Users/shihaitong/opt/anaconda3/lib/python3.8/site-packages (from tensorboard) (50.3.1.post20201107)\n",
            "Collecting absl-py>=0.4\n",
            "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 14.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting grpcio>=1.24.3\n",
            "  Downloading grpcio-1.38.0-cp38-cp38-macosx_10_10_x86_64.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 10.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
            "\u001b[K     |████████████████████████████████| 781 kB 25.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /Users/shihaitong/opt/anaconda3/lib/python3.8/site-packages (from tensorboard) (1.0.1)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 15.4 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Users/shihaitong/opt/anaconda3/lib/python3.8/site-packages (from tensorboard) (2.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /Users/shihaitong/opt/anaconda3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard) (1.15.0)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
            "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 19.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/shihaitong/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/shihaitong/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/shihaitong/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Users/shihaitong/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Collecting pyasn1>=0.1.3\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.6 MB/s eta 0:00:011\n",
            "\u001b[?25hCollecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 24.7 MB/s eta 0:00:01\n",
            "\u001b[?25hInstalling collected packages: cachetools, pyasn1, rsa, pyasn1-modules, google-auth, tensorboard-data-server, protobuf, absl-py, grpcio, tensorboard-plugin-wit, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, tensorboard\n",
            "Successfully installed absl-py-0.12.0 cachetools-4.2.2 google-auth-1.30.2 google-auth-oauthlib-0.4.4 grpcio-1.38.0 markdown-3.3.4 oauthlib-3.1.1 protobuf-3.17.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWm50g8Kd1uY"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def np_to_tensor(x, device):\n",
        "    # allocates tensors from np.arrays\n",
        "    if device == 'cpu':\n",
        "        return torch.from_numpy(x).cpu()\n",
        "    else:\n",
        "        return torch.from_numpy(x).contiguous().pin_memory().to(device=device, non_blocking=True)\n",
        "\n",
        "def accuracy_fn(y_hat, y):\n",
        "    # computes classification accuracy\n",
        "    return (y_hat.round() == y.round()).float().mean()\n",
        "\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    # dataset class that deals with loading the data and making it available by index.\n",
        "\n",
        "    def __init__(self, path, device, use_patches=True, resize_to=(400, 400)):\n",
        "        self.path = path\n",
        "        self.device = device\n",
        "        self.use_patches = use_patches\n",
        "        self.resize_to=resize_to\n",
        "        self.x, self.y, self.n_samples = None, None, None\n",
        "        self._load_data()\n",
        "\n",
        "    def _load_data(self):  # not very scalable, but good enough for now\n",
        "        self.x = load_all_from_path(os.path.join(self.path, 'images'))\n",
        "        self.y = load_all_from_path(os.path.join(self.path, 'groundtruth'))\n",
        "        if self.use_patches:  # split each image into patches\n",
        "            self.x, self.y = image_to_patches(self.x, self.y)\n",
        "        elif self.resize_to != (self.x.shape[1], self.x.shape[2]):  # resize images\n",
        "            self.x = np.stack([cv2.resize(img, dsize=self.resize_to) for img in self.x], 0)\n",
        "            self.y = np.stack([cv2.resize(mask, dsize=self.resize_to) for mask in self.y], 0)\n",
        "        self.x = np.moveaxis(self.x, -1, 1)  # pytorch works with CHW format instead of HWC\n",
        "        self.n_samples = len(self.x)\n",
        "\n",
        "    def _preprocess(self, x, y):\n",
        "        # to keep things simple we will not apply transformations to each sample,\n",
        "        # but it would be a very good idea to look into preprocessing\n",
        "        return x, y\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self._preprocess(np_to_tensor(self.x[item], self.device), np_to_tensor(self.y[[item]], self.device))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt929X-gd1uZ"
      },
      "source": [
        "class Block(nn.Module):\n",
        "    # a repeating structure composed of two convolutional layers with batch normalization and ReLU activations\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.BatchNorm2d(out_ch),\n",
        "                                   nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
        "                                   nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "        \n",
        "class UNet(nn.Module):\n",
        "    # UNet-like architecture for single class semantic segmentation.\n",
        "    def __init__(self, chs=(3,64,128,256,512,1024)):\n",
        "        super().__init__()\n",
        "        enc_chs = chs  # number of channels in the encoder\n",
        "        dec_chs = chs[::-1][:-1]  # number of channels in the decoder\n",
        "        self.enc_blocks = nn.ModuleList([Block(in_ch, out_ch) for in_ch, out_ch in zip(enc_chs[:-1], enc_chs[1:])])  # encoder blocks\n",
        "        self.pool = nn.MaxPool2d(2)  # pooling layer (can be reused as it will not be trained)\n",
        "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(in_ch, out_ch, 2, 2) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # deconvolution\n",
        "        self.dec_blocks = nn.ModuleList([Block(in_ch, out_ch) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # decoder blocks\n",
        "        self.head = nn.Sequential(nn.Conv2d(dec_chs[-1], 1, 1), nn.Sigmoid()) # 1x1 convolution for producing the output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encode\n",
        "        enc_features = []\n",
        "        for block in self.enc_blocks[:-1]:\n",
        "            x = block(x)  # pass through the block\n",
        "            enc_features.append(x)  # save features for skip connections\n",
        "            x = self.pool(x)  # decrease resolution\n",
        "        x = self.enc_blocks[-1](x)\n",
        "        # decode\n",
        "        for block, upconv, feature in zip(self.dec_blocks, self.upconvs, enc_features[::-1]):\n",
        "            x = upconv(x)  # increase resolution\n",
        "            x = torch.cat([x, feature], dim=1)  # concatenate skip features\n",
        "            x = block(x)  # pass through the block\n",
        "        return self.head(x)  # reduce to 1 channel\n",
        "\n",
        "\n",
        "def patch_accuracy_fn(y_hat, y):\n",
        "    # computes accuracy weighted by patches (metric used on Kaggle for evaluation)\n",
        "    h_patches = y.shape[-2] // PATCH_SIZE\n",
        "    w_patches = y.shape[-1] // PATCH_SIZE\n",
        "    patches_hat = y_hat.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
        "    patches = y.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
        "    return (patches == patches_hat).float().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rto5jNbId1ua"
      },
      "source": [
        "def train(train_dataloader, eval_dataloader, model, loss_fn, metric_fns, optimizer, n_epochs):\n",
        "    # training loop\n",
        "    logdir = './tensorboard/net'\n",
        "    writer = SummaryWriter(logdir)  # tensorboard writer (can also log images)\n",
        "\n",
        "    history = {}  # collects metrics at the end of each epoch\n",
        "\n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        # initialize metric list\n",
        "        metrics = {'loss': [], 'val_loss': []}\n",
        "        for k, _ in metric_fns.items():\n",
        "            metrics[k] = []\n",
        "            metrics['val_'+k] = []\n",
        "\n",
        "        pbar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{n_epochs}')\n",
        "        # training\n",
        "        model.train()\n",
        "        for (x, y) in pbar:\n",
        "            optimizer.zero_grad()  # zero out gradients\n",
        "            y_hat = model(x)  # forward pass\n",
        "            loss = loss_fn(y_hat, y)\n",
        "            loss.backward()  # backward pass\n",
        "            optimizer.step()  # optimize weights\n",
        "\n",
        "            # log partial metrics\n",
        "            metrics['loss'].append(loss.item())\n",
        "            for k, fn in metric_fns.items():\n",
        "                metrics[k].append(fn(y_hat, y).item())\n",
        "            pbar.set_postfix({k: sum(v)/len(v) for k, v in metrics.items() if len(v) > 0})\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():  # do not keep track of gradients\n",
        "            for (x, y) in eval_dataloader:\n",
        "                y_hat = model(x)  # forward pass\n",
        "                loss = loss_fn(y_hat, y)\n",
        "                \n",
        "                # log partial metrics\n",
        "                metrics['val_loss'].append(loss.item())\n",
        "                for k, fn in metric_fns.items():\n",
        "                    metrics['val_'+k].append(fn(y_hat, y).item())\n",
        "\n",
        "        # summarize metrics, log to tensorboard and display\n",
        "        history[epoch] = {k: sum(v) / len(v) for k, v in metrics.items()}\n",
        "        for k, v in history[epoch].items():\n",
        "          writer.add_scalar(k, v, epoch)\n",
        "        print(' '.join(['\\t- '+str(k)+' = '+str(v)+'\\n ' for (k, v) in history[epoch].items()]))\n",
        "        #show_val_samples(x.detach().cpu().numpy(), y.detach().cpu().numpy(), y_hat.detach().cpu().numpy())\n",
        "\n",
        "    print('Finished Training')\n",
        "    # plot loss curves\n",
        "    plt.plot([v['loss'] for k, v in history.items()], label='Training Loss')\n",
        "    plt.plot([v['val_loss'] for k, v in history.items()], label='Validation Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "8ef35d3b10cc4fe2a6f0a97037330db7",
            "1126b5213e9c45f8bbdd2f9d24392b84"
          ]
        },
        "id": "tQwg2-1td1ua",
        "collapsed": true,
        "outputId": "b5949cb8-d1e7-4ce0-f834-d3c363fe59d7"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# reshape the image to simplify the handling of skip connections and maxpooling\n",
        "train_dataset = ImageDataset('training', device, use_patches=False, resize_to=(384, 384))\n",
        "val_dataset = ImageDataset('validation', device, use_patches=False, resize_to=(384, 384))\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "model = UNet().to(device)\n",
        "loss_fn = nn.BCELoss()\n",
        "metric_fns = {'acc': accuracy_fn, 'patch_acc': patch_accuracy_fn}\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "n_epochs = 35\n",
        "train(train_dataloader, val_dataloader, model, loss_fn, metric_fns, optimizer, n_epochs)\n",
        "\n",
        "# predict on test set\n",
        "test_filenames = (glob(test_path + '/*.png'))\n",
        "test_images = load_all_from_path(test_path)\n",
        "batch_size = test_images.shape[0]\n",
        "size = test_images.shape[1:3]\n",
        "# we also need to resize the test images. This might not be the best ideas depending on their spatial resolution.\n",
        "test_images = np.stack([cv2.resize(img, dsize=(384, 384)) for img in test_images], 0)\n",
        "test_images = np_to_tensor(np.moveaxis(test_images, -1, 1), device)\n",
        "test_pred = [model(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
        "test_pred = np.concatenate(test_pred, 0)\n",
        "test_pred= np.moveaxis(test_pred, 1, -1)  # CHW to HWC\n",
        "test_pred = np.stack([cv2.resize(img, dsize=size) for img in test_pred], 0)  # resize to original shape\n",
        "# now compute labels\n",
        "test_pred = test_pred.reshape((-1, size[0] // PATCH_SIZE, PATCH_SIZE, size[0] // PATCH_SIZE, PATCH_SIZE))\n",
        "test_pred = np.moveaxis(test_pred, 2, 3)\n",
        "test_pred = np.round(np.mean(test_pred, (-1, -2)) > CUTOFF)\n",
        "create_submission(test_pred, test_filenames, submission_filename='unet_submission.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ef35d3b10cc4fe2a6f0a97037330db7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Epoch 1/35'), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t- loss = 0.46858016649882\n",
            "  \t- val_loss = 0.7403407990932465\n",
            "  \t- acc = 0.7733028580745062\n",
            "  \t- val_acc = 0.30035994946956635\n",
            "  \t- patch_acc = 0.6143301526705424\n",
            "  \t- val_patch_acc = 0.2924262136220932\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1126b5213e9c45f8bbdd2f9d24392b84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(HTML(value='Epoch 2/35'), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2093749784fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_fns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# predict on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-c55fb96fc9cf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, eval_dataloader, model, loss_fn, metric_fns, optimizer, n_epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# zero out gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-ce5a57e02bec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# increase resolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# concatenate skip features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pass through the block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# reduce to 1 channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-ce5a57e02bec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}