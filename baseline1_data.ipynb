{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "baseline1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JE39xqqSx6m"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from glob import glob\n",
        "from random import sample\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fp9ZtEUTMOs"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKncWHn4TbJd"
      },
      "source": [
        "!unzip /content/drive/MyDrive/cil-road-segmentation-2021.zip -d /content/drive/MyDrive/cil-road-segmentation-2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKOmAnQ-fpvP"
      },
      "source": [
        "def random_rotate_img(img, angle):\n",
        "    (h, w) = img.shape[:2]\n",
        "    (cX, cY) = (w // 2, h // 2)\n",
        "    # grab the rotation matrix (applying the negative of the\n",
        "    # angle to rotate clockwise), then grab the sine and cosine\n",
        "    # (i.e., the rotation components of the matrix)\n",
        "    M = cv2.getRotationMatrix2D((cX, cY), -angle, 1.0)\n",
        "    cos = np.abs(M[0, 0])\n",
        "    sin = np.abs(M[0, 1])\n",
        "  \n",
        "    # compute the new bounding dimensions of the image\n",
        "    nW = int((h * sin) + (w * cos))\n",
        "    nH = int((h * cos) + (w * sin))\n",
        "  \n",
        "    # adjust the rotation matrix to take into account translation\n",
        "    M[0, 2] += (nW / 2) - cX\n",
        "    M[1, 2] += (nH / 2) - cY\n",
        "    # perform the actual rotation and return the image\n",
        "    # return tf.convert_to_tensor(cv2.warpAffine(img, M, (nW, nH),borderValue=(0,0,0)))\n",
        "    return tf.image.resize_image_with_crop_or_pad(cv2.warpAffine(img, M, (nW, nH),borderValue=(0,0,0)),400,400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2oJg3DZacfE"
      },
      "source": [
        "scale_up_factor = 1.2\n",
        "scale_down_factor = 0.8\n",
        "\n",
        "# flip the image horizaontally with probability = prob\n",
        "def horizontal_flip(train_img, label_img, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "        return tf.image.random_flip_left_right(train_img), tf.image.random_flip_left_right(label_img)\n",
        "    else:\n",
        "        return tf.convert_to_tensor(train_img), tf.convert_to_tensor(label_img)\n",
        "\n",
        "# flip the image vertically with probability = prob\n",
        "def vertical_flip(train_img, label_img, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "        return tf.image.random_flip_up_down(train_img), tf.image.random_flip_up_down(label_img)\n",
        "    else:\n",
        "        return tf.convert_to_tensor(train_img), tf.convert_to_tensor(label_img)\n",
        "\n",
        "# rotate the image by k*90 degree with probability = prob\n",
        "def rotate_90s(train_img, label_img, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "        # 1<= k <= 3, rotate clockwise by 90/180/270 degree\n",
        "        k = np.random.randint(low=1, high=4, size=1)[0]\n",
        "        return tf.image.rot90(train_img, k), tf.image.rot90(label_img, k)\n",
        "    else:\n",
        "        return tf.convert_to_tensor(train_img), tf.convert_to_tensor(label_img)\n",
        "\n",
        "# rotate the image by a minor degree in [+25, -25] with probability = prob\n",
        "def random_rotate(train_img, label_img, min_angle = -25, max_angle = 25, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "      random_angle = np.random.uniform(min_angle, max_angle, 1)[0]\n",
        "      return random_rotate_img(train_img, random_angle), random_rotate_img(label_img, random_angle)\n",
        "    else:\n",
        "      return tf.convert_to_tensor(train_img), tf.convert_to_tensor(label_img)\n",
        "\n",
        "# scale up by 1.2 with probability = prob1(then randomly crop), scale down by 0.8 with probability = prob2(with padding), keep unchanged with probability = prob3.\n",
        "# prob1 + prob2 + prob3 = 1\n",
        "def scale_up_down(train_img, label_img, probs=[0.65, 0.9, 1]):\n",
        "    rdn = np.random.random()\n",
        "    cur_height = img.shape[0]\n",
        "    cur_width = img.shape[1]\n",
        "    if rdn < probs[0]:\n",
        "        # scale up\n",
        "        # img_t = tf.image.crop_and_resize(train_img, [[1,1,1,1]], [0], (int(cur_height*scale_up_factor), int(cur_width*scale_up_factor)))\n",
        "        # img_l = tf.image.crop_and_resize(label_img, [[1,1,1,1]], [0], (int(cur_height*scale_up_factor), int(cur_width*scale_up_factor)))     \n",
        "        img_t = tf.image.resize(train_img, (int(cur_height*scale_up_factor), int(cur_width*scale_up_factor)))\n",
        "        img_l = tf.image.resize(label_img, (int(cur_height*scale_up_factor), int(cur_width*scale_up_factor)))     \n",
        "        img_tt = tf.image.resize_image_with_crop_or_pad(img_t,400,400)\n",
        "        img_ll = tf.image.resize_image_with_crop_or_pad(img_l,400,400)\n",
        "        return img_tt, img_ll\n",
        "    elif rdn < probs[1]:\n",
        "        # scale down\n",
        "        # img_t = tf.image.crop_and_resize(train_img, [[1,1,1,1]], [0], (int(cur_height*scale_down_factor), int(cur_width*scale_down_factor)))\n",
        "        # img_l = tf.image.crop_and_resize(label_img, [[1,1,1,1]], [0], (int(cur_height*scale_down_factor), int(cur_width*scale_down_factor)))     \n",
        "        img_t = tf.image.resize(train_img, (int(cur_height*scale_down_factor), int(cur_width*scale_down_factor)))\n",
        "        img_l = tf.image.resize(label_img, (int(cur_height*scale_down_factor), int(cur_width*scale_down_factor))) \n",
        "        img_tt = tf.image.resize_image_with_crop_or_pad(img_t,400,400)\n",
        "        img_ll = tf.image.resize_image_with_crop_or_pad(img_l,400,400)\n",
        "        return img_tt, img_ll\n",
        "    # else remain unchanged\n",
        "    return tf.convert_to_tensor(train_img), tf.convert_to_tensor(label_img)\n",
        "\n",
        "# adjust the hue of an RGB image by random factor in [0, 0.5] with probability = prob\n",
        "def hue_image(train_img, label_img, min_hue_factor=0, max_hue_factor=0.5, seed=248, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "        random_factor = np.random.uniform(min_hue_factor, max_hue_factor, 1)[0]\n",
        "        return tf.image.random_hue(train_img, random_factor, seed=seed), tf.convert_to_tensor(label_img)\n",
        "    else:\n",
        "        return tf.convert_to_tensor(train_img), tf.convert_to_tensor(label_img)\n",
        "\n",
        "# adjust the brightness of an RGB image by random factor in [1, 1.1] with probability = prob\n",
        "def brightness_image(train_img, label_img, min_brightness_factor=1 ,max_brightness_factor=1.1, seed=248, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "        random_factor = np.random.uniform(min_brightness_factor, max_brightness_factor, 1)[0]\n",
        "        return tf.image.random_brightness(train_img, random_factor, seed=seed), tf.convert_to_tensor(label_img)\n",
        "    else:\n",
        "        return tf.convert_to_tensor(train_img), tf.convert_to_tensor(label_img)\n",
        "\n",
        "# adjust the saturation of an RGB image by random factor in [1, 1.1] with probability = prob\n",
        "def saturation_image(train_img, label_img, min_saturation_factor=1, max_saturation_factor=1.1, seed=248, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "        return tf.image.random_saturation(train_img, min_saturation_factor, max_saturation_factor, seed=seed), tf.convert_to_tensor(label_img)\n",
        "    else:\n",
        "        return tf.convert_to_tensor(train_img), tf.convert_to_tensor(label_img)\n",
        "\n",
        "# adjust the contrast of an RGB image by random factor in [1, 1.1] with probability = prob\n",
        "def contrast_image(train_img, label_img, min_contrast_factor=1, max_contrast_factor=1.1, seed=248, prob=0.75):\n",
        "    rdn = np.random.random()\n",
        "    if rdn < prob:\n",
        "        return tf.image.random_contrast(train_img, min_contrast_factor, max_contrast_factor, seed=seed), tf.convert_to_tensor(label_img)\n",
        "    else:\n",
        "        return tf.convert_to_tensor(train_img), tf.convert_to_tensor(label_img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQSXmm5G6Xc6"
      },
      "source": [
        "def normalize(image):\n",
        "    mean = np.mean(image)\n",
        "    var = np.mean(np.square(image-mean))\n",
        "    image = (image - mean)/np.sqrt(var)\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9phwC6nna4Zq"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "train_img = cv2.imread('/content/drive/MyDrive/cil-road-segmentation-2021/training/training/images/satImage_001.png')\n",
        "label_img = cv2.imread('/content/drive/MyDrive/cil-road-segmentation-2021/training/training/groundtruth/satImage_001.png')\n",
        "imgs = np.hstack([train_img,label_img])\n",
        "cv2_imshow(imgs)\n",
        "\n",
        "# result = np.zeros(train_img.shape, dtype=np.float32)\n",
        "# normal_img = cv2.normalize(train_img, result, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "# cv2_imshow(normal_img)\n",
        "img_t,img_l = random_rotate(train_img, label_img)\n",
        "with tf.Session() as sess:\n",
        "    result_t = sess.run(img_t)\n",
        "    result_l = sess.run(img_l)\n",
        "    res_imgs = np.hstack([result_t,result_l])\n",
        "    cv2_imshow(res_imgs)\n",
        "    # cv2_imshow(result_t)\n",
        "    # cv2_imshow(result_l)\n",
        "# cv2_imshow(img_t)\n",
        "# cv2_imshow(img_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoEFsuB4Sx6q"
      },
      "source": [
        "# some constants\n",
        "PATCH_SIZE = 16  # pixels per side of square patches\n",
        "VAL_SIZE = 10  # size of the validation set (number of images)\n",
        "CUTOFF = 0.25  # minimum average brightness for a mask patch to be classified as containing road\n",
        "\n",
        "# unzip the dataset, split it and organize it in folders\n",
        "if not os.path.isdir('validation'):  # make sure this has not been executed yet\n",
        "  try:\n",
        "          #!unzip cil-road-segmentation-2021.zip\n",
        "          !mv training/training/* training\n",
        "          !rm -rf training/training\n",
        "          !mkdir validation\n",
        "          !mkdir validation/images\n",
        "          !mkdir validation/groundtruth\n",
        "          for img in sample(glob(\"training/images/*.png\"), VAL_SIZE):\n",
        "            os.rename(img, img.replace('training', 'validation'))\n",
        "            mask = img.replace('images', 'groundtruth')\n",
        "            os.rename(mask, mask.replace('training', 'validation'))\n",
        "  except:\n",
        "      print('Please upload a .zip file containing your datasets.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGpi99JASx6r"
      },
      "source": [
        "def load_all_from_path(path):\n",
        "    # loads all HxW .pngs contained in path as a 4D np.array of shape (n_images, H, W, 3)\n",
        "    # images are loaded as floats with values in the interval [0., 1.]\n",
        "    return np.stack([np.array(Image.open(f)) for f in sorted(glob(path + '/*.png'))]).astype(np.float32) / 255.\n",
        "\n",
        "# paths to training and validation datasets\n",
        "train_path = 'training'\n",
        "val_path = 'validation'\n",
        "\n",
        "train_images = load_all_from_path(os.path.join(train_path, 'images'))\n",
        "train_masks = load_all_from_path(os.path.join(train_path, 'groundtruth'))\n",
        "val_images = load_all_from_path(os.path.join(val_path, 'images'))\n",
        "val_masks = load_all_from_path(os.path.join(val_path, 'groundtruth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JHi8cjfSx6r"
      },
      "source": [
        "print(train_images.shape)\n",
        "print(train_images[-2][-1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY16_IIDSx6t"
      },
      "source": [
        "def image_to_patches(images, masks=None):\n",
        "    # takes in a 4D np.array containing images and (optionally) a 4D np.array containing the segmentation masks\n",
        "    # returns a 4D np.array with an ordered sequence of patches extracted from the image and (optionally) a np.array containing labels\n",
        "    n_images = images.shape[0]  # number of images\n",
        "    h, w = images.shape[1:3]  # shape of images\n",
        "    assert (h % PATCH_SIZE) + (w % PATCH_SIZE) == 0  # make sure images can be patched exactly\n",
        "\n",
        "    h_patches = h // PATCH_SIZE\n",
        "    w_patches = w // PATCH_SIZE\n",
        "    patches = images.reshape((n_images, h_patches, PATCH_SIZE, h_patches, PATCH_SIZE, -1))\n",
        "    patches = np.moveaxis(patches, 2, 3)\n",
        "    patches = patches.reshape(-1, PATCH_SIZE, PATCH_SIZE, 3)\n",
        "    if masks is None:\n",
        "        return patches\n",
        "\n",
        "    masks = masks.reshape((n_images, h_patches, PATCH_SIZE, h_patches, PATCH_SIZE, -1))\n",
        "    masks = np.moveaxis(masks, 2, 3)\n",
        "    labels = np.mean(masks, (-1, -2, -3)) > CUTOFF  # compute labels\n",
        "    labels = labels.reshape(-1).astype(np.float32)\n",
        "    return patches, labels\n",
        "\n",
        "train_patches, train_labels = image_to_patches(train_images, train_masks)\n",
        "val_patches, val_labels = image_to_patches(val_images, val_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH7eZL-MSx6u"
      },
      "source": [
        "print(train_patches.shape)\n",
        "print(train_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyyhhvqCSx6w"
      },
      "source": [
        "pip install tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMkJo0nZSx6w"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def np_to_tensor(x, device):\n",
        "    # allocates tensors from np.arrays\n",
        "    if device == 'cpu':\n",
        "        return torch.from_numpy(x).cpu()\n",
        "    else:\n",
        "        return torch.from_numpy(x).contiguous().pin_memory().to(device=device, non_blocking=True)\n",
        "\n",
        "def accuracy_fn(y_hat, y):\n",
        "    # computes classification accuracy\n",
        "    return (y_hat.round() == y.round()).float().mean()\n",
        "\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    # dataset class that deals with loading the data and making it available by index.\n",
        "\n",
        "    def __init__(self, path, device, use_patches=True, resize_to=(400, 400)):\n",
        "        self.path = path\n",
        "        self.device = device\n",
        "        self.use_patches = use_patches\n",
        "        self.resize_to=resize_to\n",
        "        self.x, self.y, self.n_samples = None, None, None\n",
        "        self._load_data()\n",
        "\n",
        "    def _load_data(self):  # not very scalable, but good enough for now\n",
        "        self.x = load_all_from_path(os.path.join(self.path, 'images'))\n",
        "        self.y = load_all_from_path(os.path.join(self.path, 'groundtruth'))\n",
        "        if self.use_patches:  # split each image into patches\n",
        "            self.x, self.y = image_to_patches(self.x, self.y)\n",
        "        elif self.resize_to != (self.x.shape[1], self.x.shape[2]):  # resize images\n",
        "            self.x = np.stack([cv2.resize(img, dsize=self.resize_to) for img in self.x], 0)\n",
        "            self.y = np.stack([cv2.resize(mask, dsize=self.resize_to) for mask in self.y], 0)\n",
        "        self.x = np.moveaxis(self.x, -1, 1)  # pytorch works with CHW format instead of HWC\n",
        "        self.n_samples = len(self.x)\n",
        "\n",
        "    def _preprocess(self, x, y):\n",
        "        # to keep things simple we will not apply transformations to each sample,\n",
        "        # but it would be a very good idea to look into preprocessing\n",
        "        return x, y\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self._preprocess(np_to_tensor(self.x[item], self.device), np_to_tensor(self.y[[item]], self.device))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s-4fJ46Sx6y"
      },
      "source": [
        "class Block(nn.Module):\n",
        "    # a repeating structure composed of two convolutional layers with batch normalization and ReLU activations\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.BatchNorm2d(out_ch),\n",
        "                                   nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=3, padding=1),\n",
        "                                   nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "        \n",
        "class UNet(nn.Module):\n",
        "    # UNet-like architecture for single class semantic segmentation.\n",
        "    def __init__(self, chs=(3,64,128,256,512,1024)):\n",
        "        super().__init__()\n",
        "        enc_chs = chs  # number of channels in the encoder\n",
        "        dec_chs = chs[::-1][:-1]  # number of channels in the decoder\n",
        "        self.enc_blocks = nn.ModuleList([Block(in_ch, out_ch) for in_ch, out_ch in zip(enc_chs[:-1], enc_chs[1:])])  # encoder blocks\n",
        "        self.pool = nn.MaxPool2d(2)  # pooling layer (can be reused as it will not be trained)\n",
        "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(in_ch, out_ch, 2, 2) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # deconvolution\n",
        "        self.dec_blocks = nn.ModuleList([Block(in_ch, out_ch) for in_ch, out_ch in zip(dec_chs[:-1], dec_chs[1:])])  # decoder blocks\n",
        "        self.head = nn.Sequential(nn.Conv2d(dec_chs[-1], 1, 1), nn.Sigmoid()) # 1x1 convolution for producing the output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encode\n",
        "        enc_features = []\n",
        "        for block in self.enc_blocks[:-1]:\n",
        "            x = block(x)  # pass through the block\n",
        "            enc_features.append(x)  # save features for skip connections\n",
        "            x = self.pool(x)  # decrease resolution\n",
        "        x = self.enc_blocks[-1](x)\n",
        "        # decode\n",
        "        for block, upconv, feature in zip(self.dec_blocks, self.upconvs, enc_features[::-1]):\n",
        "            x = upconv(x)  # increase resolution\n",
        "            x = torch.cat([x, feature], dim=1)  # concatenate skip features\n",
        "            x = block(x)  # pass through the block\n",
        "        return self.head(x)  # reduce to 1 channel\n",
        "\n",
        "\n",
        "def patch_accuracy_fn(y_hat, y):\n",
        "    # computes accuracy weighted by patches (metric used on Kaggle for evaluation)\n",
        "    h_patches = y.shape[-2] // PATCH_SIZE\n",
        "    w_patches = y.shape[-1] // PATCH_SIZE\n",
        "    patches_hat = y_hat.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
        "    patches = y.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
        "    return (patches == patches_hat).float().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJa7THIISx6z"
      },
      "source": [
        "def train(train_dataloader, eval_dataloader, model, loss_fn, metric_fns, optimizer, n_epochs):\n",
        "    # training loop\n",
        "    logdir = './tensorboard/net'\n",
        "    writer = SummaryWriter(logdir)  # tensorboard writer (can also log images)\n",
        "\n",
        "    history = {}  # collects metrics at the end of each epoch\n",
        "\n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        # initialize metric list\n",
        "        metrics = {'loss': [], 'val_loss': []}\n",
        "        for k, _ in metric_fns.items():\n",
        "            metrics[k] = []\n",
        "            metrics['val_'+k] = []\n",
        "\n",
        "        pbar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{n_epochs}')\n",
        "        # training\n",
        "        model.train()\n",
        "        for (x, y) in pbar:\n",
        "            optimizer.zero_grad()  # zero out gradients\n",
        "            y_hat = model(x)  # forward pass\n",
        "            loss = loss_fn(y_hat, y)\n",
        "            loss.backward()  # backward pass\n",
        "            optimizer.step()  # optimize weights\n",
        "\n",
        "            # log partial metrics\n",
        "            metrics['loss'].append(loss.item())\n",
        "            for k, fn in metric_fns.items():\n",
        "                metrics[k].append(fn(y_hat, y).item())\n",
        "            pbar.set_postfix({k: sum(v)/len(v) for k, v in metrics.items() if len(v) > 0})\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():  # do not keep track of gradients\n",
        "            for (x, y) in eval_dataloader:\n",
        "                y_hat = model(x)  # forward pass\n",
        "                loss = loss_fn(y_hat, y)\n",
        "                \n",
        "                # log partial metrics\n",
        "                metrics['val_loss'].append(loss.item())\n",
        "                for k, fn in metric_fns.items():\n",
        "                    metrics['val_'+k].append(fn(y_hat, y).item())\n",
        "\n",
        "        # summarize metrics, log to tensorboard and display\n",
        "        history[epoch] = {k: sum(v) / len(v) for k, v in metrics.items()}\n",
        "        for k, v in history[epoch].items():\n",
        "          writer.add_scalar(k, v, epoch)\n",
        "        print(' '.join(['\\t- '+str(k)+' = '+str(v)+'\\n ' for (k, v) in history[epoch].items()]))\n",
        "        #show_val_samples(x.detach().cpu().numpy(), y.detach().cpu().numpy(), y_hat.detach().cpu().numpy())\n",
        "\n",
        "    print('Finished Training')\n",
        "    # plot loss curves\n",
        "    plt.plot([v['loss'] for k, v in history.items()], label='Training Loss')\n",
        "    plt.plot([v['val_loss'] for k, v in history.items()], label='Validation Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEScJufKSx60"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# reshape the image to simplify the handling of skip connections and maxpooling\n",
        "train_dataset = ImageDataset('training', device, use_patches=False, resize_to=(384, 384))\n",
        "val_dataset = ImageDataset('validation', device, use_patches=False, resize_to=(384, 384))\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "model = UNet().to(device)\n",
        "loss_fn = nn.BCELoss()\n",
        "metric_fns = {'acc': accuracy_fn, 'patch_acc': patch_accuracy_fn}\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "n_epochs = 35\n",
        "train(train_dataloader, val_dataloader, model, loss_fn, metric_fns, optimizer, n_epochs)\n",
        "\n",
        "# predict on test set\n",
        "test_filenames = (glob(test_path + '/*.png'))\n",
        "test_images = load_all_from_path(test_path)\n",
        "batch_size = test_images.shape[0]\n",
        "size = test_images.shape[1:3]\n",
        "# we also need to resize the test images. This might not be the best ideas depending on their spatial resolution.\n",
        "test_images = np.stack([cv2.resize(img, dsize=(384, 384)) for img in test_images], 0)\n",
        "test_images = np_to_tensor(np.moveaxis(test_images, -1, 1), device)\n",
        "test_pred = [model(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
        "test_pred = np.concatenate(test_pred, 0)\n",
        "test_pred= np.moveaxis(test_pred, 1, -1)  # CHW to HWC\n",
        "test_pred = np.stack([cv2.resize(img, dsize=size) for img in test_pred], 0)  # resize to original shape\n",
        "# now compute labels\n",
        "test_pred = test_pred.reshape((-1, size[0] // PATCH_SIZE, PATCH_SIZE, size[0] // PATCH_SIZE, PATCH_SIZE))\n",
        "test_pred = np.moveaxis(test_pred, 2, 3)\n",
        "test_pred = np.round(np.mean(test_pred, (-1, -2)) > CUTOFF)\n",
        "create_submission(test_pred, test_filenames, submission_filename='unet_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsrds2nkSx61"
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "print(input, input.shape)\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "print(target, target.shape)\n",
        "output = loss(input, target)\n",
        "print(output)\n",
        "output.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qf03LTPSx62"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}