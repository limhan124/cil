{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from random import sample\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some constants\n",
    "PATCH_SIZE = 16  # pixels per side of square patches\n",
    "VAL_SIZE = 10  # size of the validation set (number of images)\n",
    "CUTOFF = 0.25  # minimum average brightness for a mask patch to be classified as containing road\n",
    "\n",
    "# unzip the dataset, split it and organize it in folders\n",
    "if not os.path.isdir('validation'):  # make sure this has not been executed yet\n",
    "  try:\n",
    "          #!unzip cil-road-segmentation-2021.zip\n",
    "          !mv training/training/* training\n",
    "          !rm -rf training/training\n",
    "          !mkdir validation\n",
    "          !mkdir validation/images\n",
    "          !mkdir validation/groundtruth\n",
    "          for img in sample(glob(\"training/images/*.png\"), VAL_SIZE):\n",
    "            os.rename(img, img.replace('training', 'validation'))\n",
    "            mask = img.replace('images', 'groundtruth')\n",
    "            os.rename(mask, mask.replace('training', 'validation'))\n",
    "  except:\n",
    "      print('Please upload a .zip file containing your datasets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_from_path(path):\n",
    "    # loads all HxW .pngs contained in path as a 4D np.array of shape (n_images, H, W, 3)\n",
    "    # images are loaded as floats with values in the interval [0., 1.]\n",
    "    return np.stack([np.array(Image.open(f)) for f in sorted(glob(path + '/*.png'))]).astype(np.float32) / 255.\n",
    "\n",
    "# paths to training and validation datasets\n",
    "train_path = 'training'\n",
    "val_path = 'validation'\n",
    "\n",
    "train_images = load_all_from_path(os.path.join(train_path, 'images'))\n",
    "train_masks = load_all_from_path(os.path.join(train_path, 'groundtruth'))\n",
    "val_images = load_all_from_path(os.path.join(val_path, 'images'))\n",
    "val_masks = load_all_from_path(os.path.join(val_path, 'groundtruth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_patches(images, masks=None):\n",
    "    # takes in a 4D np.array containing images and (optionally) a 4D np.array containing the segmentation masks\n",
    "    # returns a 4D np.array with an ordered sequence of patches extracted from the image and (optionally) a np.array containing labels\n",
    "    n_images = images.shape[0]  # number of images\n",
    "    h, w = images.shape[1:3]  # shape of images\n",
    "    assert (h % PATCH_SIZE) + (w % PATCH_SIZE) == 0  # make sure images can be patched exactly\n",
    "\n",
    "    h_patches = h // PATCH_SIZE\n",
    "    w_patches = w // PATCH_SIZE\n",
    "    patches = images.reshape((n_images, h_patches, PATCH_SIZE, h_patches, PATCH_SIZE, -1))\n",
    "    patches = np.moveaxis(patches, 2, 3)\n",
    "    patches = patches.reshape(-1, PATCH_SIZE, PATCH_SIZE, 3)\n",
    "    if masks is None:\n",
    "        return patches\n",
    "\n",
    "    masks = masks.reshape((n_images, h_patches, PATCH_SIZE, h_patches, PATCH_SIZE, -1))\n",
    "    masks = np.moveaxis(masks, 2, 3)\n",
    "    labels = np.mean(masks, (-1, -2, -3)) > CUTOFF  # compute labels\n",
    "    labels = labels.reshape(-1).astype(np.float32)\n",
    "    return patches, labels\n",
    "\n",
    "train_patches, train_labels = image_to_patches(train_images, train_masks)\n",
    "val_patches, val_labels = image_to_patches(val_images, val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "def np_to_tensor(x, device):\n",
    "    # allocates tensors from np.arrays\n",
    "    if device == 'cpu':\n",
    "        return torch.from_numpy(x).cpu()\n",
    "    else:\n",
    "        return torch.from_numpy(x).contiguous().pin_memory().to(device=device, non_blocking=True)\n",
    "\n",
    "def accuracy_fn(y_hat, y):\n",
    "    # computes classification accuracy\n",
    "    return (y_hat.round() == y.round()).float().mean()\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    # dataset class that deals with loading the data and making it available by index.\n",
    "\n",
    "    def __init__(self, path, device, use_patches=True, resize_to=(400, 400)):\n",
    "        self.path = path\n",
    "        self.device = device\n",
    "        self.use_patches = use_patches\n",
    "        self.resize_to=resize_to\n",
    "        self.x, self.y, self.n_samples = None, None, None\n",
    "        self._load_data()\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        self.transform = transforms.Compose([transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "    def _load_data(self):  # not very scalable, but good enough for now\n",
    "        self.x = load_all_from_path(os.path.join(self.path, 'images'))\n",
    "        self.y = load_all_from_path(os.path.join(self.path, 'groundtruth'))\n",
    "        if self.use_patches:  # split each image into patches\n",
    "            self.x, self.y = image_to_patches(self.x, self.y)\n",
    "        elif self.resize_to != (self.x.shape[1], self.x.shape[2]):  # resize images\n",
    "            self.x = np.stack([cv2.resize(img, dsize=self.resize_to) for img in self.x], 0)\n",
    "            self.y = np.stack([cv2.resize(mask, dsize=self.resize_to) for mask in self.y], 0)\n",
    "        self.x = np.moveaxis(self.x, -1, 1)  # pytorch works with CHW format instead of HWC\n",
    "        self.n_samples = len(self.x)\n",
    "\n",
    "    def _preprocess(self, x, y):\n",
    "        # to keep things simple we will not apply transformations to each sample,\n",
    "        # but it would be a very good idea to look into preprocessing\n",
    "        # print(\"preprocess x, y shape\", x.shape, y.shape)\n",
    "        x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self._preprocess(np_to_tensor(self.x[item], self.device), np_to_tensor(self.y[[item]], self.device))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_accuracy_fn(y_hat, y):\n",
    "    # computes accuracy weighted by patches (metric used on Kaggle for evaluation)\n",
    "    h_patches = y.shape[-2] // PATCH_SIZE\n",
    "    w_patches = y.shape[-1] // PATCH_SIZE\n",
    "    patches_hat = y_hat.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
    "    patches = y.reshape(-1, 1, h_patches, PATCH_SIZE, w_patches, PATCH_SIZE).mean((-1, -3)) > CUTOFF\n",
    "    return (patches == patches_hat).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import model.resnet as models\n",
    "\n",
    "import importlib\n",
    "importlib.reload(models)\n",
    "\n",
    "class PPM(nn.Module):\n",
    "    def __init__(self, in_dim, reduction_dim, bins):\n",
    "        super(PPM, self).__init__()\n",
    "        self.features = []\n",
    "        for bin in bins:\n",
    "            self.features.append(nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(bin),\n",
    "                nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(reduction_dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "        self.features = nn.ModuleList(self.features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_size = x.size()\n",
    "        out = [x]\n",
    "        for f in self.features:\n",
    "            out.append(F.interpolate(f(x), x_size[2:], mode='bilinear', align_corners=True))\n",
    "        return torch.cat(out, 1)\n",
    "\n",
    "\n",
    "class PSPNet(nn.Module):\n",
    "    def __init__(self, layers=50, bins=(1, 2, 3, 6), dropout=0.1, classes=2, zoom_factor=8, use_ppm=True, criterion=nn.CrossEntropyLoss(ignore_index=-1), pretrained=True, train = False):\n",
    "        super(PSPNet, self).__init__()\n",
    "        assert layers in [50, 101, 152]\n",
    "        assert 2048 % len(bins) == 0\n",
    "        assert classes > 1\n",
    "        assert zoom_factor in [1, 2, 4, 8]\n",
    "        self.zoom_factor = zoom_factor\n",
    "        self.use_ppm = use_ppm\n",
    "        self.criterion = criterion\n",
    "        self.training = train\n",
    "\n",
    "        if layers == 50:\n",
    "            resnet = models.resnet50(pretrained=pretrained)\n",
    "        elif layers == 101:\n",
    "            resnet = models.resnet101(pretrained=pretrained)\n",
    "        else:\n",
    "            resnet = models.resnet152(pretrained=pretrained)\n",
    "        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.conv2, resnet.bn2, resnet.relu, resnet.conv3, resnet.bn3, resnet.relu, resnet.maxpool)\n",
    "        self.layer1, self.layer2, self.layer3, self.layer4 = resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4\n",
    "\n",
    "        for n, m in self.layer3.named_modules():\n",
    "            if 'conv2' in n:\n",
    "                m.dilation, m.padding, m.stride = (2, 2), (2, 2), (1, 1)\n",
    "            elif 'downsample.0' in n:\n",
    "                m.stride = (1, 1)\n",
    "        for n, m in self.layer4.named_modules():\n",
    "            if 'conv2' in n:\n",
    "                m.dilation, m.padding, m.stride = (4, 4), (4, 4), (1, 1)\n",
    "            elif 'downsample.0' in n:\n",
    "                m.stride = (1, 1)\n",
    "\n",
    "        fea_dim = 2048\n",
    "        if use_ppm:\n",
    "            self.ppm = PPM(fea_dim, int(fea_dim/len(bins)), bins)\n",
    "            fea_dim *= 2\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Conv2d(fea_dim, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=dropout),\n",
    "            nn.Conv2d(512, classes, kernel_size=1)\n",
    "        )\n",
    "        #if self.training:\n",
    "        self.aux = nn.Sequential(\n",
    "            nn.Conv2d(1024, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=dropout),\n",
    "            nn.Conv2d(256, classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x_size = x.size()\n",
    "        # assert (x_size[2]-1) % 8 == 0 and (x_size[3]-1) % 8 == 0\n",
    "        h = int((x_size[2] - 1) / 8 * self.zoom_factor + 1)\n",
    "        w = int((x_size[3] - 1) / 8 * self.zoom_factor + 1)\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x_tmp = self.layer3(x)\n",
    "        x = self.layer4(x_tmp)\n",
    "        if self.use_ppm:\n",
    "            x = self.ppm(x)\n",
    "        x = self.cls(x)\n",
    "        #if self.zoom_factor != 1:\n",
    "            # x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=True)\n",
    "        x = F.interpolate(x, scale_factor=8, mode='bilinear', align_corners=True)\n",
    "        #print(\"pspnet x shape\", x.shape)\n",
    "        #print(x)\n",
    "        \n",
    "        if self.training:\n",
    "            aux = self.aux(x_tmp)\n",
    "            #if self.zoom_factor != 1:\n",
    "            aux = F.interpolate(aux, scale_factor=8, mode='bilinear', align_corners=True)\n",
    "            print(\"pspnet aux shape\", aux.shape)\n",
    "            print(\"x.max(1)[1]\", x.max(1)[1].shape)\n",
    "            #print(x.max(1)[1])\n",
    "            main_loss = self.criterion(x, y)\n",
    "            aux_loss = self.criterion(aux, y)\n",
    "            return x.max(1)[1], main_loss, aux_loss\n",
    "        else:\n",
    "            aux = self.aux(x_tmp)\n",
    "            aux = F.interpolate(aux, scale_factor=8, mode='bilinear', align_corners=True)\n",
    "            main_loss = self.criterion(x, y)\n",
    "            aux_loss = self.criterion(aux, y)\n",
    "            return x.max(1)[1], main_loss, aux_loss\n",
    "            #return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, eval_dataloader, model, loss_fn, metric_fns, optimizer, n_epochs):\n",
    "    # training loop\n",
    "    logdir = './tensorboard/net'\n",
    "    writer = SummaryWriter(logdir)  # tensorboard writer (can also log images)\n",
    "\n",
    "    history = {}  # collects metrics at the end of each epoch\n",
    "\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        # initialize metric list\n",
    "        metrics = {'loss': [], 'val_loss': []}\n",
    "        for k, _ in metric_fns.items():\n",
    "            metrics[k] = []\n",
    "            metrics['val_'+k] = []\n",
    "\n",
    "        pbar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{n_epochs}')\n",
    "        # training\n",
    "        model.train()\n",
    "        for (x, y) in pbar:\n",
    "            optimizer.zero_grad()  # zero out gradients\n",
    "            y = y.squeeze(1).long() #(8, 400, 400)\n",
    "            print(\"x, y shape\", x.shape, y.shape)\n",
    "            y_hat, main_loss, aux_loss = model(x, y)  # forward pass\n",
    "            # loss = loss_fn(y_hat, y)\n",
    "            loss = main_loss + 0.4 * aux_loss\n",
    "            loss.backward()  # backward pass\n",
    "            optimizer.step()  # optimize weights\n",
    "\n",
    "            # log partial metrics\n",
    "            metrics['loss'].append(loss.item())\n",
    "            for k, fn in metric_fns.items():\n",
    "                metrics[k].append(fn(y_hat.float(), y.float()).item())\n",
    "            pbar.set_postfix({k: sum(v)/len(v) for k, v in metrics.items() if len(v) > 0})\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # do not keep track of gradients\n",
    "            for (x, y) in eval_dataloader:\n",
    "                y = y.squeeze(1).long()\n",
    "                y_hat, main_loss, aux_loss = model(x, y)  # forward pass\n",
    "                loss = main_loss + 0.4 * aux_loss\n",
    "                # loss = loss_fn(y_hat, y)\n",
    "                \n",
    "                # log partial metrics\n",
    "                metrics['val_loss'].append(loss.item())\n",
    "                for k, fn in metric_fns.items():\n",
    "                    metrics['val_'+k].append(fn(y_hat.float(), y.float()).item())\n",
    "\n",
    "        # summarize metrics, log to tensorboard and display\n",
    "        history[epoch] = {k: sum(v) / len(v) for k, v in metrics.items()}\n",
    "        for k, v in history[epoch].items():\n",
    "          writer.add_scalar(k, v, epoch)\n",
    "        print(' '.join(['\\t- '+str(k)+' = '+str(v)+'\\n ' for (k, v) in history[epoch].items()]))\n",
    "        #show_val_samples(x.detach().cpu().numpy(), y.detach().cpu().numpy(), y_hat.detach().cpu().numpy())\n",
    "\n",
    "    print('Finished Training')\n",
    "    # plot loss curves\n",
    "    plt.plot([v['loss'] for k, v in history.items()], label='Training Loss')\n",
    "    plt.plot([v['val_loss'] for k, v in history.items()], label='Validation Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring \"Error(s) in loading state_dict for ResNet:\n",
      "\tMissing key(s) in state_dict: \"conv2.weight\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"conv3.weight\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\". \n",
      "\tsize mismatch for conv1.weight: copying a param with shape torch.Size([64, 3, 7, 7]) from checkpoint, the shape in current model is torch.Size([64, 3, 3, 3]).\n",
      "\tsize mismatch for layer1.0.conv1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 128, 1, 1]).\n",
      "\tsize mismatch for layer1.0.downsample.0.weight: copying a param with shape torch.Size([256, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5792c1e4139c40649f6d8639a07bc01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1/35'), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([2, 3, 400, 400]) torch.Size([2, 400, 400])\n",
      "pspnet aux shape torch.Size([2, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([2, 400, 400])\n",
      "\n",
      "\t- loss = 0.442318048949043\n",
      "  \t- val_loss = 5.378546953201294\n",
      "  \t- acc = 0.908760741353035\n",
      "  \t- val_acc = 1.0\n",
      "  \t- patch_acc = 0.8953666637341181\n",
      "  \t- val_patch_acc = 1.0\n",
      " \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edcf1807f73e4dc8a602c2000f3f15fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 2/35'), FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n",
      "x, y shape torch.Size([8, 3, 400, 400]) torch.Size([8, 400, 400])\n",
      "pspnet aux shape torch.Size([8, 2, 400, 400])\n",
      "x.max(1)[1] torch.Size([8, 400, 400])\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# reshape the image to simplify the handling of skip connections and maxpooling\n",
    "train_dataset = ImageDataset('training', device, use_patches=False, resize_to=(400, 400))\n",
    "val_dataset = ImageDataset('validation', device, use_patches=False, resize_to=(400, 400))\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "model = PSPNet(layers=101, bins=(1, 2, 3, 6), dropout=0.1, classes=2, zoom_factor=1, use_ppm=True, pretrained=True, train = True).to(device)\n",
    "#loss_fn = nn.BCELoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "metric_fns = {'acc': accuracy_fn, 'patch_acc': patch_accuracy_fn}\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "n_epochs = 35\n",
    "train(train_dataloader, val_dataloader, model, loss_fn, metric_fns, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "test_filenames = (glob(test_path + '/*.png'))\n",
    "test_images = load_all_from_path(test_path)\n",
    "batch_size = test_images.shape[0]\n",
    "size = test_images.shape[1:3]\n",
    "# we also need to resize the test images. This might not be the best ideas depending on their spatial resolution.\n",
    "test_images = np.stack([cv2.resize(img, dsize=(384, 384)) for img in test_images], 0)\n",
    "test_images = np_to_tensor(np.moveaxis(test_images, -1, 1), device)\n",
    "test_pred = [model(t).detach().cpu().numpy() for t in test_images.unsqueeze(1)]\n",
    "test_pred = np.concatenate(test_pred, 0)\n",
    "test_pred= np.moveaxis(test_pred, 1, -1)  # CHW to HWC\n",
    "test_pred = np.stack([cv2.resize(img, dsize=size) for img in test_pred], 0)  # resize to original shape\n",
    "# now compute labels\n",
    "test_pred = test_pred.reshape((-1, size[0] // PATCH_SIZE, PATCH_SIZE, size[0] // PATCH_SIZE, PATCH_SIZE))\n",
    "test_pred = np.moveaxis(test_pred, 2, 3)\n",
    "test_pred = np.round(np.mean(test_pred, (-1, -2)) > CUTOFF)\n",
    "create_submission(test_pred, test_filenames, submission_filename='unet_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
